# Deep Learning: Theory and Practice
## 前言
记录学习深度学习的主要内容，理论与实践并重，将着重介绍学习前馈神经网络、循环神经网络、卷积神经网络的内容以及将深度学习应用于推荐系统和NLP的诸多理论和实践，作为自己学习历程的点滴记录，也为大家学习相应的内容提供参考，不足之处请多多指教，共同进步。

## 传统机器学习模型

- Perceptron
- Logistic Regression
- Softmax Regression (Multinomial Logistic Regression)

## 循环神经网络


## 参考

1. **[EE-559 – DEEP LEARNING (SPRING 2019)](https://fleuret.org/ee559/),[François Fleuret](http://www.idiap.ch/~fleuret/) at the [EPFL,](http://www.epfl.ch/) Switzerland. **

   This course covers the main deep learning tools and theoretical results, with examples in the [PyTorch](http://pytorch.org/) framework.

2. **Stanford-CS-230-deep-learning super-cheatsheet**

   Includes three parts:    Convolutional Neural Networks、Recurrent Neural Networks、Tips and tricks.

   [super-cheatsheet-deep-learning](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#architecture)

3. ~~to do....~~



   ### 


## Reading Lists

### RNNs

#### gradient vanishing & gradient exploding about RNN

- Pascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua. "On the difficulty of training Recurrent Neural Networks." 52.3(2012):III-1310. [链接](http://proceedings.mlr.press/v28/pascanu13.pdf)

#### GRU

- Cho, Kyunghyun, et al. "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation." Computer Science (2014). [链接](https://arxiv.org/pdf/1406.1078v3.pdf)
- Cho, Kyunghyun, et al. "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches." Computer Science (2014). [链接](https://arxiv.org/pdf/1409.1259.pdf)
- Chung, Junyoung, et al. "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling." Eprint Arxiv (2014). [链接](https://arxiv.org/pdf/1412.3555.pdf)

#### LSTM

- Hochreiter S, Schmidhuber J. Long Short-Term Memory[J]. Neural Computation, 1997, 9(8):1735-1780. [链接](https://www.bioinf.jku.at/publications/older/2604.pdf)

#### word2vec

- Mikolov T, Chen K, Corrado G, et al. Efficient Estimation of Word Representations in Vector Space[J]. Computer Science, 2013. [链接](https://arxiv.org/pdf/1301.3781.pdf)
- Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and their Compositionality[J]. 2013, 26:3111-3119. [链接](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation Conference on Empirical Methods in Natural Language Processing. 2014:1532-1543. [链接](https://www.aclweb.org/anthology/D14-1162)

#### seq2seq

- Sutskever I, Vinyals O, Le Q V. Sequence to Sequence Learning with Neural Networks[J]. 2014. [链接](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)
- Cho K, Van Merrienboer B, Gulcehre C, et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation[J]. Computer Science, 2014. [链接](https://arxiv.org/pdf/1406.1078.pdf)

#### attention mechanism

- Attention Is All You Need. Ashish Vaswani, et al. [链接](https://arxiv.org/pdf/1706.03762.pdf)
- Neural Machine Translation by Jointly Learning to Align and Translate. Bahdanau, et al. [链接](https://arxiv.org/pdf/1409.0473.pdf)

#### BLEU score

- BLEU: a Method for Automatic Evaluation of Machine Translation. Papineni, et al. 2002. [链接](https://aclanthology.info/pdf/P/P02/P02-1040.pdf)

